---
title: "EDA Tasks by Kaggle Project-5"
Names: Anjan Kumar Regulavalasa Dominique Miranda Tarun Gulati Jenisha Rawal
date: "2023-06-18"
output:
  pdf_document: default
  html_document: default
---
# Table of Contents
> Introduction to Business Problem
> Task 1: Explore the target variable in application_{train|test}.csv. Is the data unbalanced with respect to the target?
> Task 2: Explore relationship between target and predictors, looking for potentially strong predictors that could be included later in a model.
> Task 3: Loading the skimr package and the janitor package for data exploration and data cleaning.
> Task 4: Exploration of Missing data Scope in train|test Data.
> Task 5: Checking for Columns with near-zero or zero variance and outliers.
> Task 6: Will the input data need to be transformed in order to be used in a model? 
> Task 7: Joining application_train.csv with transactional data in, for example, bureau.csv or previous_application.csv.
> Task 8: Exploration of joined transactional data.
> Results: What does the data signify?
> Group Members Contribution

>Introduction to Business Problem: A majority of the loan applications are rejected because of insufficient or non-existent credit histories of the applicant, who are forced to turn to untrustworthy lenders for their financial needs. To address this issue, Home Credit uses both Telco Data as well as Transactional Data, to predict the loan repayment abilities of the applicants. If an applicant is deemed fit and capable to repay a loan, his application is accepted or otherwise rejected.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Task 1: Explore the target variable in application_{train|test}.csv. Is the data unbalanced with respect to the target? 

```{r}
library(tidyverse)
library(ggplot2)
library(caret)
library(e1071)
test <- read.csv(file = "application_test.csv", stringsAsFactors = TRUE)
train <- read.csv(file = "application_train.csv", stringsAsFactors = TRUE)

str(train)
summary(train)

str(test)
summary(test)

```

```{r}
barplot(table(train$TARGET),col="blue")
prop.table(table(train$TARGET))
simple_accuracy_reject <- train %>% summarize(credit_default = mean(TARGET == 0)) 
simple_accuracy_reject
simple_accuracy_accept <- train %>% summarize(credit_default = mean(TARGET == 1))
simple_accuracy_accept
# unbalanced because it is skewed, as (target==0) has a proportion of approximately 0.92, and (target==1) has a proportion of approximately 0.08. The accuracy of the model with the majority class classifier is 91.92%.

```

>With respect to the target, the data is unbalanced because it is skewed.

#Task 2: Explore relationship between target and predictors, looking for potentially strong predictors that could be included later in a model.

```{r}
ggplot(train, aes(x = factor(TARGET), y = AMT_CREDIT)) + 
  geom_boxplot() +
  labs(title = "TARGET ~ AMT_CREDIT")

ggplot(train,aes(x=AMT_INCOME_TOTAL))+
  geom_histogram(fill="darkblue") +
  xlab("Income") +  
  ylab("Frequency") + 
  ggtitle("Histogram of Income")

barplot(table(train$TARGET, train$NAME_CONTRACT_TYPE))

ggplot(train, aes(x = TARGET, col = NAME_CONTRACT_TYPE)) + geom_density() + labs(title = "TARGET ~ NAME_CONTRACT_TYPE")
```

```{r}
#correlation
library(psych)
# Read the data

# Specify the target variable and predictors
pairs.panels(train[2:9])
pairs.panels(test[2:9])
```

>Potentially strong predictors are NAME_CONTRACT_TYPE,AMT_INCOME_TOTAL, and AMT_CREDIT.

# Task-3 Loading the skimr package and the janitor package for data exploration and data cleaning.

```{r}
# install.packages("skimr")
library(skimr)

# install.packages("janitor")
library(janitor)

# install.packages("readr")
library(readr)

summary <- skim(train)
print(summary)

```

# Task-4 Exploration of Missing data Scope in train|test Data.
```{r warning = FALSE}

# Explore missing data wihtin the training dataset

train_missing_data <- skimr::skim(train)
train_missing_data <- arrange(train_missing_data, desc(n_missing))
print(train_missing_data)

# Explore missing data within the testing dataset

test_missing_data <- skimr::skim(test)
test_missing_data <- arrange(test_missing_data, desc(n_missing))
print(test_missing_data)

```

```{r}
# Calculation of missing values percentage in each column within the training dataset

train_missing_percentages <- train_missing_data %>%
  janitor::tabyl(everything()) %>%
  janitor::adorn_percentages("col")

# Calculation of missing values percentage in each column within the testing dataset

test_missing_percentages <- test_missing_data %>%
  janitor::tabyl(everything()) %>%
  janitor::adorn_percentages("col")

# Print columns with missing data and their percentages for training data
print(train_missing_percentages)

# Print columns with missing data and their percentages for test data
print(test_missing_percentages)
```
> These percentages indicate the proportion of missing values within each column. Higher percentages suggest that a larger portion of the data in those columns is missing.

```{r}
# Impute missing values with 0 in the 'OWN_CAR_AGE' column
train <- train %>%
  replace_na(list(OWN_CAR_AGE = 0))
View(train)
print(head(train))
```

```{r}

# Removal of columns with high percentage of missing data in train dataset

na_counts <- apply(train, 2, function(x) sum(is.na(x)))
missing_percent <- na_counts / nrow(train)
cols_to_remove <- names(missing_percent[missing_percent > 0.5])
train_clean_data <- train[, !(names(train) %in% cols_to_remove)]
print(head(train_clean_data))

```

```{r}
# Extraction of the "OWN_CAR_AGE" column
OWN_CAR_AGE <- train_clean_data$OWN_CAR_AGE

# Defining the breaks and labels
breaks <- c(-Inf, 0, 5, 10, Inf)
labels <- c("0", "0-5", "5-10", "10+")

# Create bins
OWN_CAR_AGE_BINS <- cut(OWN_CAR_AGE, breaks = breaks, labels = labels)

# Append OWN_CAR_AGE_BINS to train dataset
train_clean_data <- cbind(train_clean_data, OWN_CAR_AGE_BINS)

print(head(train_clean_data[, c("OWN_CAR_AGE_BINS", "OWN_CAR_AGE")], n=10))

```

> Here, columns that have more than 50% of missing values have been removed, expect 'OWN_CAR_AGE' column. The null values of this column has been replaced by '0'. 
> Also, "OWN_CAR_AGE_BINS" column has been created for better interpretation of the "OWN_CAR_AGE" column.
> After cleaning and exploring the missing data, we have 307,511 rows and 86 columns.

# Task 5: Checking for Columns with near-zero or zero variance and outliers
```{r warning = FALSE}

# Check for columns with near-zero variance
zero_var_cols <- names(train_clean_data)[apply(train_clean_data, 2, var) < 0.001] 

# Output columns with near-zero variance
if (length(zero_var_cols) > 0) {
  cat("Columns with near-zero variance:", paste(zero_var_cols, collapse = ", "), "\n")
} else {
  cat("No columns with near-zero variance found.\n")
}
```
```{r warning = FALSE}
# Check for columns with zero variance
zero_var_cols <- names(train_clean_data)[apply(train_clean_data, 2, var) == 0] 

# Output columns with zero variance
if (length(zero_var_cols) == 0) {
  cat("Columns with zero variance:", paste(zero_var_cols, collapse = ", "), "\n")
} else {
  cat("No columns with zero variance found.\n")
}
```

```{r warning = FALSE}

# Calculation of the outliers for each column

set.seed(123)
threshold <- 3
num_outliers <- numeric()

for (col_name in colnames(train_clean_data)) {
  column <- train_clean_data[[col_name]]
  
  if (is.factor(column)) {
    num_outliers[col_name] <- sum(all(duplicated(column)[-1L]))
  } else {
    lower_bound <- mean(column) - threshold * sd(column)
    upper_bound <- mean(column) + threshold * sd(column)
    outliers <- column[column < lower_bound | column > upper_bound]
    num_outliers[col_name] <- length(outliers)
  }
}

for (col_name in names(num_outliers)) {
  cat("Column", col_name, ":", num_outliers[col_name], "outliers\n")
}
```

> In this scenario, it is stated that the target variable has 24,825 outliers. However, it is important to clarify that since the variable is binary, meaning it can only take values of 0 or 1, the concept of outliers does not apply in this context. The treatment and inclusion of these outlier outputs will be appropriately addressed during the modeling phase.
> Additionally, there are no columns with zero variance. However, there are few columns with near-zero variance.

# Task 6: Will the input data need to be transformed in order to be used in a model?

```{r}
train_cat_variables <- names(train_clean_data)[sapply(train_clean_data, is.factor)]

# Listing of the categorical variables
train_cat_variables
```
> Yes, the input data needs to be transformed inorder to used in a model. This involves preprocessing input data for model utilization which typically encompasses several essential tasks:

> 1.Handling missing values: This involves addressing the presence of missing data, which can entail imputing the missing values or removing rows or columns with incomplete information. The missing values of columns were addressed by replacing null values in "OWN_CAR_AGE" and by removal of columns with high percentage of missing values.
> 2.Encoding categorical variables: The dataset contains categorical variables which suggests to convert them into numerical format to ensure compatibility with the model. This process may involve techniques such as one-hot encoding and other appropriate methods.
> 3.Scaling or normalizing numeric variables: In certain situations, numeric variables may require scaling or normalization to bring them into a uniform range. This step ensures that the magnitudes of these variables do not disproportionately influence the model's performance.

# Task 7: Joining application_train.csv with transactional data in, for example, bureau.csv or previous_application.csv.

```{r}
bureau <- read.csv("bureau.csv")

# Aggregate transactional data (bureau.csv) to have the same grain as application data
bureau_agg <- bureau %>%
  group_by(SK_ID_CURR) %>%
  summarise(
    bureau_count = n(),
    bureau_days_credit_mean = mean(DAYS_CREDIT),
    bureau_days_credit_min = min(DAYS_CREDIT),
    bureau_days_credit_max = max(DAYS_CREDIT)
  )

# Join aggregated transactional data (bureau.csv) with application data
train_bureau_agg <- left_join(train_clean_data, bureau_agg, by = "SK_ID_CURR")
print(head(train_bureau_agg))
```
```{r warning = False}
previous_application <- read.csv("previous_application.csv")

# Aggregate transactional data (previous_application.csv) to have the same grain as application data
previous_agg <- previous_application %>%
  group_by(SK_ID_CURR) %>%
  summarise(
    prev_count = n(),
    prev_amt_credit_mean = mean(AMT_CREDIT),
    prev_amt_credit_min = min(AMT_CREDIT),
    prev_amt_credit_max = max(AMT_CREDIT)
  )

# Join aggregated transactional data (previous_application.csv) with application data
train_previous_agg <- left_join(train_clean_data, previous_agg, by = "SK_ID_CURR")
print(head(train_previous_agg))
```

# Task 8:Exploration of joined transactional data

```{r warning = FALSE}

# Exploration of the joined transactional data of bureau.
cor(train_bureau_agg[, c("TARGET", "bureau_days_credit_mean", "bureau_days_credit_min", "bureau_days_credit_max")])

# Analyzing the missing values and outliers
summary(train_bureau_agg[, c("bureau_days_credit_mean", "bureau_days_credit_min", "bureau_days_credit_max")])

# Creation of new features
train_bureau_agg$bureau_credit_ratio <- train_bureau_agg$bureau_days_credit_mean / train_bureau_agg$AMT_CREDIT

```

```{r warning = FALSE}

# Exploration of the joined transactional data of previous_applications.
cor(train_previous_agg[, c("TARGET", "prev_amt_credit_mean", "prev_amt_credit_min", "prev_amt_credit_max")])

# Analyzing the missing values and outliers
summary(train_previous_agg[, c("prev_amt_credit_mean", "prev_amt_credit_min", "prev_amt_credit_max")])

# Creation of new features
train_previous_agg$prev_annuity_ratio <- train_previous_agg$AMT_ANNUITY / train_previous_agg$AMT_CREDIT
```

> Let's explore the added columns and their potential for predicting default:

> Bureau_credit_ratio: This column is created in the app_bureau_agg data frame by dividing the mean number of days of credit in the bureau dataset (bureau_days_credit_mean) by the AMT_CREDIT in the application dataset. It calculates the ratio of credit duration to the amount of credit. This feature could potentially be useful in predicting default as it captures the relationship between credit duration and the loan amount.

> prev_annuity_ratio: This column is created in the app_previous_agg data frame by dividing the AMT_ANNUITY by the AMT_CREDIT in the previous_application dataset. It calculates the ratio of the annuity payment to the loan amount in the previous application. This feature might provide insights into the affordability of the loan and its potential impact on default.

> By creating these new features, the code aims to capture additional information from the transactional data that could be relevant for predicting default in the application data. These features consider aspects such as credit duration, loan amount, annuity payment, and their ratios, which could provide valuable insights for modeling default risk.

# Results: What does the data signify?

> Data Preprocessing and Feature Selection: The dataset related to loan applications and loan repayment abilities undergoes data preprocessing and feature selection. Not all columns are included in the model, as some have high null values or insignificant contributions. Manipulation and transformation of certain columns are performed to ensure their correct impact on predictions.

> Addressing the Business Problem: The business problem at hand is the rejection of loan applications due to insufficient credit histories, leading applicants to seek loans from untrustworthy lenders. To tackle this issue, Home Credit utilizes Telco Data and Transactional Data to predict loan repayment abilities. This prediction aids in making informed decisions on accepting or rejecting loan applications.

> Unbalanced Data and Accuracy: The dataset is characterized as unbalanced since the target variable, which likely indicates loan repayment ability, is skewed. The majority class (target == 0) has a proportion of approximately 0.92, while the minority class (target == 1) has a proportion of approximately 0.08. This class imbalance impacts the model's accuracy, with the majority class classifier achieving 91.92% accuracy.

> Potential Predictors and Data Exploration: Three columns, NAME_CONTRACT_TYPE, AMT_INCOME_TOTAL, and AMT_CREDIT, are identified as potentially strong predictors for loan repayment abilities. These columns are expected to significantly influence the model's predictions. Data exploration involves cleaning steps such as removing columns with high missing values, replacing null values, and creating additional columns for improved interpretation. The resulting dataset consists of 307,511 rows and 86 columns.

> In summary, the provided information highlights the importance of data preprocessing and feature selection in a loan application dataset. It addresses the business problem of insufficient credit histories leading to loan rejections, utilizing Telco Data and Transactional Data for predicting loan repayment abilities. The unbalanced nature of the data and the accuracy of the majority class classifier are emphasized. Potential predictors and data exploration steps, including cleaning and the creation of additional columns, are also discussed, aimed at enhancing the model's ability to predict loan repayment abilities and default risk.

